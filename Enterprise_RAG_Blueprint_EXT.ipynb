{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/runf21/NIM_RAG/blob/main/Enterprise_RAG_Blueprint_EXT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enterprise RAG Blueprint\n",
        "\n",
        "\n",
        "<img src =https://developer-blogs.nvidia.com/zh-cn-blog/wp-content/uploads/sites/2/2023/11/GenAI-Promo-AWS-DevNews-PRESS-1920x1080-1-960x540.png>\n"
      ],
      "metadata": {
        "id": "7-9VRTi4YFEU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "The advent of generative AI has sparked a revolution across industries, opening doors to innovations that were once the realm of science fiction.\n",
        "\n",
        "At the forefront of this movement is a class of deep learning algorithms called a Large Language Model (LLM). These LLMs can recognize, summarize, translate, predict, and generate content after being trained on massive datasets.\n",
        "\n",
        "Despite their extraordinary potential, implementing AI applications at scale presents significant challenges, demanding cutting-edge infrastructure and specialized expertise.\n",
        "\n",
        "NVIDIA addresses these challenges head-on. As a pioneer in AI computing, NVIDIA has created a comprehensive ecosystem to efficiently build, train, and deploy LLMs. Today, we'll explore this ecosystem while constructing a Retrieval-Augmented Generation (RAG) system - an advanced LLM-powered enterprise chatbot."
      ],
      "metadata": {
        "id": "ioUtdQbzozxW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NVIDIA NGC"
      ],
      "metadata": {
        "id": "Zfkd-b7suebG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the core of the NVIDIA software platform is the NVIDIA GPU cloud (NGC) - a cloud-based platform that provides access to GPU-optimized AI software, including pre-trained models, frameworks, and SDKs. It serves as a hub for AI developers to discover, download, and deploy GPU-accelerated software for their AI workflows.\n",
        "\n",
        "<img src=\"https://i.ibb.co/TB4dL4BL/NGC-website.png\" width=\"100%\">\n",
        "\n",
        "NGC streamlines the development process by offering containerized applications and enterprise-grade support, allowing teams to quickly implement AI solutions without building everything from scratch."
      ],
      "metadata": {
        "id": "hXJGFfcdutHr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Foundational Models and NIMs\n",
        "NGC hosts NVIDIA's Foundational Models - a collection of curated, pre-trained models that serve as building blocks for AI applications. These models are optimized by NVIDIA to deliver the best performance and deployed as a NVIDIA Inference Microservice (NIM). These NIMs package models into containerized, production-ready services with standardized APIs, dramatically simplifying deployment and scaling.\n",
        "\n",
        "<img src=\"https://i.ibb.co/cd85XDM/NVIDIA-API-catalog.png\" width=\"100%\">\n",
        "\n",
        "NVIDIA hosted deployments of NIMs are available to test on the [NVIDIA Build page](https://build.nvidia.com/). After testing, NIMs can be exported from NVIDIA’s API catalog using the NVIDIA AI Enterprise license and run on-premises or in the cloud, giving enterprises ownership and full control of their IP and AI application.\n",
        "\n",
        "<img src=\"https://i.ibb.co/v6RPbrBw/NVIDIA-build-demo.png\" width=\"100%\">"
      ],
      "metadata": {
        "id": "ep5ALewwxQjS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prerequisites"
      ],
      "metadata": {
        "id": "Ww1N5K-rrRqs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using this notebook\n",
        "Google Colab is a free, cloud-based platform that allows you to write and execute Python code in a Jupyter notebook environment directly from your browser. It provides access to powerful computing resources, including GPUs, making it ideal for machine learning, data analysis, and educational projects.\n",
        "\n",
        "To save your changes in this notebook, you will need to make your own copy.\n",
        "\n",
        "The notebook consists of cells where you can write and run Python code. To execute the code in a cell, simply click on the cell and press the \"Play\" button or use `Shift + Enter`."
      ],
      "metadata": {
        "id": "AZ0zS9jZrVSC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Dependencies"
      ],
      "metadata": {
        "id": "HBrLeNmYbPGh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's install all the required libraries and dependencies we need for our chatbot to work correctly. A core dependency is **LangChain** - a popular software framework that provides modular components for building LLM applications."
      ],
      "metadata": {
        "id": "n8ueNkMmbYHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# NumPy fix\n",
        "desired_version = \"1.26.4\"\n",
        "\n",
        "try:\n",
        "    import numpy as np\n",
        "    current_version = np.__version__\n",
        "    print(f\"Current NumPy version: {current_version}\")\n",
        "\n",
        "    if current_version != desired_version:\n",
        "        print(f\"Installing NumPy version {desired_version}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", f\"numpy=={desired_version}\"])\n",
        "\n",
        "        print(\"Restarting runtime to apply changes...\")\n",
        "        os.kill(os.getpid(), 9)\n",
        "    else:\n",
        "        print(\"NumPy is already the desired version.\")\n",
        "\n",
        "except ImportError:\n",
        "    print(\"NumPy is not installed. Installing...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", f\"numpy=={desired_version}\"])\n",
        "    os.kill(os.getpid(), 9)\n"
      ],
      "metadata": {
        "id": "9vRDoMiTTVYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --quiet \\\n",
        "    chainlit==1.3.2 \\\n",
        "    chromadb==0.5.20 \\\n",
        "    dataclasses-json==0.6.7 \\\n",
        "    fastapi==0.115.5 \\\n",
        "    kaleido==0.2.1 \\\n",
        "    langchain==0.3.0 \\\n",
        "    langchain-chroma==0.1.4 \\\n",
        "    langchain-community==0.3.0 \\\n",
        "    langchain-nvidia-ai-endpoints==0.3.5 \\\n",
        "    langchain-unstructured==0.1.6 \\\n",
        "    protobuf==4.25.2 \\\n",
        "    pydantic==2.9.2 \\\n",
        "    pymupdf==1.25.3 \\\n",
        "    \"unstructured[all-docs]\"==0.17.2"
      ],
      "metadata": {
        "id": "p--hLFjmbgmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll also install some system dependencies to support data processing for our data loader. These packages enable document handling, text extraction, and optical character recognition (OCR)."
      ],
      "metadata": {
        "id": "UJ9ckOMjbvzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get -qq install libmagic-dev poppler-utils tesseract-ocr libreoffice > /dev/null"
      ],
      "metadata": {
        "id": "pipK1A_gbzLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally we'll download and install Tunnelmole - an open-source application that provides a public URL for a web server running locally. This will allow us to expose a frontend web app for the chatbot we'll create and run in this notebook."
      ],
      "metadata": {
        "id": "OsHNMTyob2lx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://install.tunnelmole.com/n3d5g/install && sudo bash install > /dev/null"
      ],
      "metadata": {
        "id": "xqxpHmw2b6FR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply for an NVIDIA NGC Account\n",
        "To use the NVIDIA AI Foundational Models and NIMs, you'll need to apply for an account on NVIDIA NGC.\n",
        "\n",
        "---\n",
        "\n",
        "### Steps\n",
        "1. Go to the [NVIDIA NGC page](https://ngc.nvidia.com/signin)\n",
        "2. Click the **Welcome Guest** icon in the top right corner of the page and select **Sign In / Sign Up**.\n",
        "\n",
        "<img src =\"https://i.ibb.co/Kx8pz3Fw/welcome-guest-signin.pnghttps://i.ibb.co/Kx8pz3Fw/welcome-guest-signin.png\" width=\"50%\">\n",
        "\n",
        "3. Enter your email address and click **Continue**.\n",
        "\n",
        "<img src =\"https://docscontent.nvidia.com/dims4/default/45077b7/2147483647/strip/true/crop/462x585+0+0/resize/924x1170!/format/webp/quality/90/?url=https%3A%2F%2Fk3-prod-nvidia-docs.s3.us-west-2.amazonaws.com%2Fbrightspot%2Fdita%2F00000195-414b-d50d-a1bd-ed5fdd100000%2Fngc%2Fgpu-cloud%2Fcommon%2Fgraphics%2Fgraphics-ngc%2Flogin-new-org.png\" width=\"40%\">\n",
        "\n",
        "4. In this step, you'll create your new account by creating a new password and confirming it. Make sure to review the NVIDIA Account Terms of Use and Privacy Policy, and click **Create Account** to accept and proceed with account creation.\n",
        "\n",
        "<img src =\"https://docscontent.nvidia.com/dims4/default/90e83cc/2147483647/strip/true/crop/668x633+0+0/resize/1336x1266!/format/webp/quality/90/?url=https%3A%2F%2Fk3-prod-nvidia-docs.s3.us-west-2.amazonaws.com%2Fbrightspot%2Fdita%2F00000195-414b-d50d-a1bd-ed5fdd100000%2Fngc%2Fgpu-cloud%2Fcommon%2Fgraphics%2Fgraphics-ngc%2Fcreate-an-account-dark.png\" width=\"50%\">\n",
        "\n",
        "5. You should shortly receive an email to verify your registration. Inside the email, click **Verify Email Address**.\n",
        "\n",
        "<img src =\"https://docscontent.nvidia.com/dims4/default/5dac44d/2147483647/strip/true/crop/605x628+0+0/resize/1210x1256!/format/webp/quality/90/?url=https%3A%2F%2Fk3-prod-nvidia-docs.s3.us-west-2.amazonaws.com%2Fbrightspot%2Fdita%2F00000195-414b-d50d-a1bd-ed5fdd100000%2Fngc%2Fgpu-cloud%2Fcommon%2Fgraphics%2Fgraphics-ngc%2Faccount-created-email.png\" width=\"50%\">\n",
        "\n",
        "6. You are automatically directed to nvidia.com and see an email verified successfully page. This window will close automatically.\n",
        "\n",
        "<img src =\"https://docscontent.nvidia.com/dims4/default/2f7c26a/2147483647/strip/true/crop/685x293+0+0/resize/1370x586!/format/webp/quality/90/?url=https%3A%2F%2Fk3-prod-nvidia-docs.s3.us-west-2.amazonaws.com%2Fbrightspot%2Fdita%2F00000195-414b-d50d-a1bd-ed5fdd100000%2Fngc%2Fgpu-cloud%2Fcommon%2Fgraphics%2Fgraphics-ngc%2Femail-verified-dark.png\" width=\"50%\">\n",
        "\n",
        "7. At the Almost done! dialog, set your communications preferences, and then click **Submit**.\n",
        "\n",
        "<img src =\"https://docscontent.nvidia.com/dims4/default/d726cec/2147483647/strip/true/crop/664x399+0+0/resize/1328x798!/format/webp/quality/90/?url=https%3A%2F%2Fk3-prod-nvidia-docs.s3.us-west-2.amazonaws.com%2Fbrightspot%2Fdita%2F00000195-414b-d50d-a1bd-ed5fdd100000%2Fngc%2Fgpu-cloud%2Fcommon%2Fgraphics%2Fgraphics-ngc%2Falmost-done-dark-slim.png\" width=\"50%\">\n",
        "\n",
        "8. Enter the password you just created to continue setting up your NVIDIA Cloud Account. (This is a required security measure).\n",
        "\n",
        "<img src =\"https://docscontent.nvidia.com/dims4/default/22fd238/2147483647/strip/true/crop/657x562+0+0/resize/1314x1124!/format/webp/quality/90/?url=https%3A%2F%2Fk3-prod-nvidia-docs.s3.us-west-2.amazonaws.com%2Fbrightspot%2Fdita%2F00000195-414b-d50d-a1bd-ed5fdd100000%2Fngc%2Fgpu-cloud%2Fcommon%2Fgraphics%2Fgraphics-ngc%2Fngc-sign-in-existing-user-dark.png\" width=\"50%\">\n",
        "\n",
        "9. Give your NVIDIA Cloud Account (NCA) a name that will help you identify it easily the next time you sign-in.\n",
        "\n",
        "<img src =\"https://docscontent.nvidia.com/dims4/default/0088a68/2147483647/strip/true/crop/603x679+0+0/resize/1206x1358!/format/webp/quality/90/?url=https%3A%2F%2Fk3-prod-nvidia-docs.s3.us-west-2.amazonaws.com%2Fbrightspot%2Fdita%2F00000195-414b-d50d-a1bd-ed5fdd100000%2Fngc%2Fgpu-cloud%2Fcommon%2Fgraphics%2Fgraphics-ngc%2Fcreate-nvidia-cloud-account.png\" width=\"50%\">\n",
        "\n",
        "10. Complete your user profile at the Set Your Profile screen, agree to the NVIDIA GPU Cloud Terms of Use, and then click Submit.\n",
        "\n",
        "<img src =\"https://docscontent.nvidia.com/dims4/default/5d44ae7/2147483647/strip/true/crop/806x577+0+0/resize/1612x1154!/format/webp/quality/90/?url=https%3A%2F%2Fk3-prod-nvidia-docs.s3.us-west-2.amazonaws.com%2Fbrightspot%2Fdita%2F00000195-414b-d50d-a1bd-ed5fdd100000%2Fngc%2Fgpu-cloud%2Fcommon%2Fgraphics%2Fgraphics-ngc%2Fset-your-profile.png\" width=\"50%\">\n",
        "\n",
        "11. Your NVIDIA NGC account is created and you are automatically redirected to your individual NGC org.\n",
        "\n",
        "<img src =\"https://docscontent.nvidia.com/dims4/default/7349fc5/2147483647/strip/true/crop/1616x856+0+0/resize/2880x1526!/format/webp/quality/90/?url=https%3A%2F%2Fk3-prod-nvidia-docs.s3.us-west-2.amazonaws.com%2Fbrightspot%2Fdita%2F00000195-414b-d50d-a1bd-ed5fdd100000%2Fngc%2Fgpu-cloud%2Fcommon%2Fgraphics%2Fgraphics-ngc%2Fngc-default-landing-page.png\" width=\"50%\">\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tjo_FwRxsxG7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating an NGC API key\n",
        "\n",
        "### Steps\n",
        "1. Navigate to the [NGC API Catalog page](https://build.nvidia.com/)\n",
        "2. Select a model.\n",
        "3. Select **Get API Key** and login if prompted.\n",
        "<img src=\"https://docs.nvidia.com/nim/large-language-models/latest/_images/build_get_api_key.png\" width=\"100%\">\n",
        "4. Select **Generate Key**\n",
        "<img src=\"https://docs.nvidia.com/nim/large-language-models/latest/_images/build_generate_key.png\" width=\"100%\">\n",
        "5. Your API key appears in the following dialog. Copy and save this API key. **IMPORTANT** Keep this somewhere safe because it will only display it once! If you lose your key, you'll have to create a new one.\n",
        "<img src=\"https://docs.nvidia.com/nim/large-language-models/latest/_images/build_copy_key.png\" width=\"100%\">"
      ],
      "metadata": {
        "id": "Wb8_BSea8xY4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enter NGC API Key\n",
        "\n",
        "Time to set our NGC API key! Run this cell, input your key in the box that shows up below, and press `ENTER` on your keyboard."
      ],
      "metadata": {
        "id": "WDhYTqI7EU89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "def set_ngc_api_key():\n",
        "    \"\"\"Prompt the user to enter an NVIDIA API key if it's not set or invalid.\"\"\"\n",
        "    while True:\n",
        "        nvapi_key = getpass.getpass(\"Enter your NVIDIA API key: \")\n",
        "\n",
        "        if nvapi_key.startswith(\"nvapi-\"):\n",
        "            os.environ[\"NVIDIA_API_KEY\"] = nvapi_key\n",
        "            print(\"NVIDIA API Key has been successfully set!\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"Invalid API key. Please try again.\")\n",
        "\n",
        "# Check if the key is already set and valid\n",
        "current_key = os.environ.get(\"NVIDIA_API_KEY\", \"\")\n",
        "\n",
        "if not current_key.startswith(\"nvapi-\"):\n",
        "    print(\"NVIDIA API Key is missing or invalid. Please enter a new key.\")\n",
        "    set_ngc_api_key()\n",
        "else:\n",
        "    print(\"NVIDIA API Key is already set.\")\n",
        "    change_key = input(\"Would you like to enter a different key? (yes/no): \").strip().lower()\n",
        "\n",
        "    if change_key in [\"yes\", \"y\"]:\n",
        "        set_ngc_api_key()\n"
      ],
      "metadata": {
        "id": "scK8jidCENOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Runtimes\n",
        "\n",
        "This notebook runs on a standard CPU runtime without requiring a local GPU. While GPUs are essential for efficient model inference, all inference in this notebook is handled through cloud-based NVIDIA Inference Microservices (NIMs) hosted on the NVIDIA NGC API catalog and accessible via our NGC API key.\n",
        "\n",
        "Your runtime should already be set up, but you can verify your runtime type in the top right corner of the navigation bar. Initially, before running any code cells, the runtime will be in a disconnected state. You can either manually activate it by clicking the runtime button or let it automatically connect when you run your first code cell.\n",
        "\n",
        "<img src=\"https://i.ibb.co/spG4Fttw/Connect-button.png\" width=\"20%\">\n",
        "\n",
        "Sometimes it's neccesary to change your runtime, you can do this by clicking the down arrow and selecting \"Change runtime type\".\n",
        "\n",
        "<img src =\"https://i.ibb.co/CM932jJ/change-runtime.png\" width=\"60%\">\n",
        "\n",
        "The NVIDIA A100 and L4 GPUs are among the best available, but they require credits which cost money. They're also far more powerful than necessary for today's purposes since our inference is being hosted by NGC. We'll opt for the free CPU runtime offered by Google instead.\n",
        "\n",
        "<img src=\"https://i.ibb.co/9fpd3ww/runtime-type.png\" width=\"60%\">"
      ],
      "metadata": {
        "id": "2GbNBmPBD4WL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the prerequisites complete, it's time to start building our application! But first - let's review the fundamental concepts of Retrieval-Augmented Generation: what it is, how it works, and the key components that power it."
      ],
      "metadata": {
        "id": "R1C6nZskc_ZP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG 101"
      ],
      "metadata": {
        "id": "ahjf41sHcY2z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Very Very Short History of RAG\n",
        "Retrieval-Augmented Generation, or RAG for short, is one of the most exciting advancements in generative AI. Patrick Lewis, lead author of the 2020 paper that coined the term, apologized for the unflattering acronym that now describes a growing family of methods across hundreds of papers and dozens of commercial services he believes represent the future of generative AI.\n",
        "“We definitely would have put more thought into the name had we known our work would become so widespread,” Lewis said in an interview from Singapore, where he was sharing his ideas with a regional conference of database developers.\n",
        "“We always planned to have a nicer sounding name, but when it came time to write the paper, no one had a better idea,” said Lewis, who now leads a RAG team at AI startup Cohere.\n",
        "\n",
        "<img src =\"https://i.ibb.co/pvmYj2G/patrick-lewis.jpg\" width=\"50%\">\n",
        "<img src =\"https://i.ibb.co/Tq81dt3/RAG-whitepaper.png\" width=\"50%\">"
      ],
      "metadata": {
        "id": "V73tJl2kcFJ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG Revealed\n",
        "Before we discuss the technical details of a RAG, let’s use a real-world analogy. Imagine a courtroom. Judges hear and decide cases based on their general understanding of the law. Sometimes a case — like a malpractice suit or a labor dispute — requires special expertise, so judges send court clerks to a law library, looking for precedents and specific cases they can cite.\n",
        "\n",
        "Like a good judge, LLMs can respond to a wide variety of human queries. But to deliver authoritative answers that cite sources, the model needs an assistant to do some research. Think of a RAG as the court clerk. It’s essentially a second neural network model that is acting as a helper for the original LLM or the judge in our example.\n",
        "\n",
        "<img src =https://i.ibb.co/4p028MS/RAG-courtroom.png width=\"90%\">"
      ],
      "metadata": {
        "id": "bMbqEbi01i6G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embedding Models and Vector Databases\n",
        "This second model is called an embedding model. It converts the knowledge base it’s been given to a numerical format known as an **embedding vector** that is stored in a **vector database**. Think of a vector as a set of coordinates like latitude and longitude on a map – a list of numbers that points to the location of a piece of information. When you use GPS on your phone to search for the nearest restaurant, an algorithm is calculating the difference between your coordinates and a list of coordinates in its database. The list of restaurants it returns have the highest closeness to your location vector. This is essentially how a vector database works in our RAG.\n",
        "\n",
        "When we query the vector database, we employ a method called Semantic Search. This data searching technique uses the intent and contextual meaning behind a query to deliver more accurate results. Unlike keyword search, which focuses on matching specific words or synonyms, semantic search captures the overall meaning of the query. By considering the context and relationships between words, it delivers results that are more relevant to what the user is actually looking for.\n",
        "\n",
        "<img src =https://i.ibb.co/kxNQbXY/Vector-database-slide.png>"
      ],
      "metadata": {
        "id": "IYerkWD3h4Y4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ReRanker Models\n",
        "While semantic search helps us find the most relevant documents based on meaning rather than exact keywords, not all retrieved results are equally useful. Some may be highly relevant, while others might only be loosely related. This is where re-ranking comes into play.\n",
        "\n",
        "Re-ranking acts as a second stage in the retrieval process, ensuring that the most important and contextually accurate documents are prioritized. By leveraging the advanced language understanding of large language models (LLMs), re-ranking refines search results, making our RAG system even more effective.\n",
        "\n",
        "<img src=\"https://i.ibb.co/zVXSHrP8/Re-Ranker-Diagram.png\" width=\"100%\">"
      ],
      "metadata": {
        "id": "DNFknGYliK-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connecting the Dots\n",
        "\n",
        "Let’s bring everything together with a high-level overview of a RAG workflow.\n",
        "\n",
        "**Phase 1: Document Ingestion**\n",
        "\n",
        "In the first phase (top section of the diagram), data is ingested from a knowledge base, which may include documents, PDFs, APIs, or structured databases. This data is preprocessed by chunking it into smaller segments before being transformed into numerical embeddings by the embedding model NIM. These embeddings are then stored in a vector database, allowing for fast and efficient retrieval later.\n",
        "\n",
        "**Phase 2: User Query & Retrieval**\n",
        "\n",
        "When a user submits a query, the system first converts it into an embedding and searches the vector database using semantic search to find the most relevant information. The retriever pulls these matching results as an initial context for the response.\n",
        "\n",
        "Some retrieved results may be more relevant than others, which is where the ReRanker NIM comes in — analyzing the retrieved chunks and reordering them to ensure the most precise and valuable information is prioritized.\n",
        "\n",
        "The LLM NIM then receives this refined context, generating a response that is more accurate, contextually relevant, and informed by the knowledge base.\n",
        "\n",
        "This approach ensures that the chatbot doesn’t just retrieve documents—it delivers the best, most relevant insights to enhance user interactions.\n",
        "\n",
        "<img src=\"https://i.ibb.co/TBDXjwG7/RAG-Re-Ranker-Diagram.png\" width=\"100%\">\n",
        "\n",
        "Now that we understand the concepts behind Retrieval-Augmented Generation (RAG), let’s build our RAGbot!\n",
        "\n",
        "**Remember you need to have run all of the cells in the prerequisites section for this to work.**"
      ],
      "metadata": {
        "id": "zkfnRUrkihXN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we understand the concepts behind Retrieval-Augmented Generation (RAG), let’s build our RAGbot! Unlike a traditional LLM-powered chatbot, a RAG bot can retrieve relevant, up-to-date information from external data sources!  \n",
        "\n",
        "**Remember you need to have run all of the cells in the prerequisites section for this to work.**\n"
      ],
      "metadata": {
        "id": "bhM55t1Rhdoh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Concept to Code: Building a RAG"
      ],
      "metadata": {
        "id": "GQbnAlscP2vI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constructing the Knowledge Base\n",
        "\n",
        "The first step in building our RAG system is to create and index the knowledge base — the backbone of its architecture. As we explored in RAG 101, effective retrieval is essential for generating high-quality responses in our final application. Once the pipeline is established, the knowledge base can be updated dynamically, but initial data ingestion must occur before the LLM can retrieve and utilize the information.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dGrA20GaZ6lK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading the data\n",
        "Let's start with our data. For today’s RAG, we’ve sourced a PDF file containing documentation on NVIDIA Base Command Manager. NVIDIA Base Command Manager (BCM) is a cluster management software for high-performance computing (HPC) environments. It enables workload scheduling, resource allocation, and system monitoring across CPU and GPU clusters. BCM supports job submission through workload managers like Slurm, PBS, and Kubernetes, and integrates with tools like Jupyter and Spark for AI and data processing.\n",
        "\n",
        "Let's download it below and check that the file is in our local directory."
      ],
      "metadata": {
        "id": "OZ8C5tqVlDI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O \"BCM_User_Manual.pdf\" -nc https://support.brightcomputing.com/manuals/10/user-manual.pdf"
      ],
      "metadata": {
        "id": "pSny0IKvcGGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://i.ibb.co/GfjJgYfx/BCM-user-file.png\" width=\"50%\">"
      ],
      "metadata": {
        "id": "MNzZsSGzcWXO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at a few sample pages from the NVIDIA Bright Cluster Manager User Manual to understand the type of content our knowledge base will be ingesting. Notice the technical content, formatting, and structure of the information - these are all elements that our RAG system will need to understand and reference correctly when answering questions.\n",
        "\n",
        "PDF documents often present unique challenges: complex elements like tables, diagrams, equations, and multi-column layouts are often lost in the extraction process. Effective RAG systems require thoughtful preprocessing of PDF content - potentially using specialized extraction techniques for tables, OCR for image-based text, and custom parsing to preserve document structure."
      ],
      "metadata": {
        "id": "_kv0cTBMf2rt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz\n",
        "from IPython.display import display, HTML\n",
        "import base64\n",
        "\n",
        "def show_specific_pages(pdf_path, page_numbers):\n",
        "    doc = fitz.open(pdf_path)\n",
        "\n",
        "    html_content = '<div style=\"display: flex; flex-direction: column; gap: 20px;\">'\n",
        "\n",
        "    for page_num in page_numbers:\n",
        "        # Adjust for 0-based indexing (subtract 1 from page number)\n",
        "        i = page_num - 1\n",
        "\n",
        "        # Skip if page number is out of range\n",
        "        if i < 0 or i >= doc.page_count:\n",
        "            continue\n",
        "\n",
        "        page = doc.load_page(i)\n",
        "        pix = page.get_pixmap()\n",
        "\n",
        "        # Convert to PNG bytes\n",
        "        png_bytes = pix.tobytes(\"png\")\n",
        "\n",
        "        # Encode the image to base64\n",
        "        img_base64 = base64.b64encode(png_bytes).decode('utf-8')\n",
        "\n",
        "        # Add page to HTML\n",
        "        html_content += f'''\n",
        "        <div style=\"border: 1px solid #ddd; padding: 10px;\">\n",
        "            <h3>Page {page_num}</h3>\n",
        "            <img src=\"data:image/png;base64,{img_base64}\" style=\"width: 50%;\">\n",
        "        </div>\n",
        "        '''\n",
        "\n",
        "    html_content += '</div>'\n",
        "    return HTML(html_content)\n",
        "\n",
        "# Show pages 9, 52, and 73\n",
        "show_specific_pages(\"BCM_User_Manual.pdf\", [9, 52, 73])"
      ],
      "metadata": {
        "id": "nB7ay96UWEVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing the Data\n",
        "\n",
        "To incorporate external data into our system, we'll first need to use a set of tools named **Document Loaders** to transform our content into structured `Document` objects, each containing both text content and its associated metadata. While we'll be using a PDF for our example today, there are Document Loaders for an ever-expanding variety of data types including plain text, Word, Powerpoint, CSV, HTML, images, and more...\n",
        "\n",
        "For our RAG build, we'll be using `Unstructured` - an open-source toolkit designed to simplify the ingestion and pre-processing of diverse data formats with a focus on optimizing data for LLMs. It can handle the whole process from creating the `Document` objects to preprocessing which we'll cover next.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "XkzOJfZ-t-YU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Splitting and Chunks\n",
        "Before we processing our new data, we need to divide our `Document` objects into smaller, manageable segments - a practice known as splitting or chunking.\n",
        "\n",
        "This prepocessing step offers several benefits, such as ensuring consistent processing of varying document lengths, overcoming input size limitations of LLMs, and improving the quality of text representations used in retrieval systems.\n",
        "\n",
        "For our splitting strategy today, we'll use a powerful approach called document-based splitting, built into the `Unstructured` toolkit. Instead of breaking cotnten at fixed intervals, this technique intelligently follows the document's natural structure - using paragraphs, sections, and other meaningful boundaries as splitting points. The resulting **chunks** will retain useful context and greatly improve the retrieval quality.\n",
        "\n",
        "If you're interested in learning more about splitting in general and the different splitting approaches, check out the deep dive section below. Otherwise, let's move on to implementing Unstructured and doing our splitting in the code block below."
      ],
      "metadata": {
        "id": "uA8hAZ5KkTzE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> #### Deep Dive: Understanding Text Splitting"
      ],
      "metadata": {
        "id": "HoNQp_erkafB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There's many ways of splitting data into chunks but usually they fall under a series of approaches:\n",
        "- **Length**: The most straightforward and intuitive approach splits documents based on length, using either character count or token count as the measuring unit.\n",
        " - **Character-based**: Splitting on the number of characters in a text\n",
        " - **Token-based**: Splitting on the number of tokens - particularly useful for staying within LLM input token limits\n",
        "- **Text Structure**: Text is naturally organized into hierarchical units such as paragraphs, sentences, and words. By leveraging this inherent structure, we can create chunks that preserve both natural language flow and semantic meaning across different levels of text granularity.\n",
        " - A `Recursive Character Text Splitter` exemplifies this approach by attempting to keep larger units like paragraphs intact but splitting when it exceeds the chunk size, moving to the next smallest unit like sentences and so forth.\n",
        "- **Document Structure**: This approach leverages a document's natural organization, splitting content along its inherent structural boundaries - like sections, chapters, paragraphs, and tables - to create semantically meaningful chunks. By following the document's natural structure, this ensures smoother information flow and preserves topic boundaries - avoiding the mid-word splits and topic merging that can occur with simpler methods.\n",
        "- **Semantic Meaning**:\n",
        "Semantic-based splitting focuses on actual *meaning* rather than arbitrary fixed rules, analyzing how content changes to determine natural breakpoints for dynamic chunks of varying sizes. Using a sliding window approach to compare consecutive groups of sentences, it measures how closely their ideas relate to each other - what we call semantic similarity. This creates cohesive chunks that naturally group related content together. While this approach definitely creates more meaningful chunks - it's slower, requires more computational resources, and is much more complex to implement than other methods."
      ],
      "metadata": {
        "id": "y4kIFR-kkgUM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using Unstructured"
      ],
      "metadata": {
        "id": "gjU0vzqqktzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unstructured.partition.pdf import partition_pdf\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "# Partition PDF\n",
        "elements = partition_pdf(\"BCM_User_Manual.pdf\", chunking_strategy=\"basic\")\n",
        "\n",
        "# Convert Unstructured elements to LangChain Documents\n",
        "documents = []\n",
        "for element in elements:\n",
        "    # Get text content\n",
        "    content = element.text if hasattr(element, 'text') else str(element)\n",
        "\n",
        "    # Get page number safely\n",
        "    page_number = None\n",
        "    if hasattr(element, 'metadata'):\n",
        "        if hasattr(element.metadata, 'page_number'):\n",
        "            page_number = element.metadata.page_number\n",
        "\n",
        "    # Create document\n",
        "    doc = Document(\n",
        "        page_content=content,\n",
        "        metadata={\n",
        "            'source': \"BCM_User_Manual.pdf\",\n",
        "            'page': page_number\n",
        "        }\n",
        "    )\n",
        "    documents.append(doc)\n",
        "\n",
        "print(f\"Loaded {len(documents)} document elements.\")"
      ],
      "metadata": {
        "id": "IDRTE6T1XZ2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assembling the Retrieval Pipeline\n",
        "Now that we've preprocessed our data chunks and built our knowledge base, it's time to focus on the core retrieval components of our RAG architecture."
      ],
      "metadata": {
        "id": "YS5LhpLP2_nD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating Embeddings\n",
        "\n",
        "Let's implement our first NIM - the embedding model. We'll use this model to vectorize our data chunks and generate vector embeddings. Embeddings are numerical representations of text that capture semantic meaning in a way computers can understand. They convert words and phrases into lists of numbers (vectors) - similar to how we can describe any location on Earth using latitude and longitude coordinates. Just as nearby places have similar coordinates, texts with similar meanings will have similar numerical values. This is crucial for RAG systems because it lets us efficiently find related content by calculating how close these coordinates are to each other.\n",
        "\n",
        "In our implementation, we'll leverage the `nv-embedqa-e5-v5` NIM from NGC, an optimized embedding model for text-based question-answering retrieval. This model, a fine-tuned variant of `E5-Large-Unsupervised`, has been trained on a public dataset to enhance retrieval accuracy and efficiency."
      ],
      "metadata": {
        "id": "OFgT1s2jaDdb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
        "\n",
        "# Create embeddings\n",
        "embedding_model = \"nvidia/nv-embedqa-e5-v5\"\n",
        "embedder = NVIDIAEmbeddings(model=embedding_model, truncate=\"END\")"
      ],
      "metadata": {
        "id": "m003DTWPrjq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Storing Embeddings in a Vector Database\n",
        "Now we'll store our newly generated embeddings in a vector database.\n",
        "\n",
        "A vector database is designed to store and manage vector embeddings - the numerical representations of data like text or images we spoke about earlier.\n",
        "\n",
        "Unlike traditional databases that store and search data based on exact matches, vector databases enable similarity searches by comparing how close data points are in a high-dimensional space. To use the previous example of coordinates on a map, when we query the vector database - we're simply finding the closest neighbors to our location and returning them.\n",
        "\n",
        "Both LangChain and NVIDIA provide support for a wide selection of vector stores. For this workflow, we've selected [**Chroma**](https://www.trychroma.com/) as our vector database. Chroma is a powerful, open-source vector database designed specifically for AI applications. While there are many other excellent vector database solutions available, we chose Chroma for it's ease of use and portability.\n",
        "\n",
        "Let’s set up Chroma and load our embeddings into the vector database."
      ],
      "metadata": {
        "id": "V-Otu7D1LH0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "import time\n",
        "\n",
        "# Create and persist vectorstore\n",
        "start_time = time.time()\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=documents,\n",
        "    embedding=embedder,\n",
        "    collection_name=\"bcm_docs\",\n",
        "    persist_directory=\"./chroma_db\"\n",
        ")\n",
        "\n",
        "if vectorstore:\n",
        "    print(f\"Vector database was successfully created! Total embeddings indexed: {len(documents)}\")\n",
        "else:\n",
        "    print(\"Failed to create the vector database. Please check your input data.\")\n",
        "\n",
        "print(f\"--- {time.time() - start_time} seconds ---\")"
      ],
      "metadata": {
        "id": "7BwBEuTLvcMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding a Reranker\n",
        "\n",
        "Now that our embeddings are stored in Chroma, we need a way to refine our search results for better accuracy.\n",
        "\n",
        "To achieve this, we'll implement a Reranker model. A Reranker reorders the retrieved results based on their relevance to the query. While our vector database efficiently finds similar embeddings, a reranker applies a more sophisticated scoring model—often transformer-based—to ensure that the most relevant matches appear at the top. Think of it as a second pass! This extra step improves the overall quality of responses, making our system more precise and contextually aware.\n",
        "\n",
        "We'll use the `nv-rerankqa-mistral-4b-v3` NIM from NGC. Based on the Mistral 4B architecture, this model is fine-tuned to score and reorder retrieved documents, ensuring that the most relevant information is prioritized. The `top n` variable refers to the number of document chunks that will be selected and reranked by the reranking model. After the initial vector retrieval returns results, the reranker will process the top 10 chunks (in this case) and rearrange them according to their relevance to the query, before passing them to the LLM."
      ],
      "metadata": {
        "id": "9jeOs9_ankrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_nvidia_ai_endpoints import NVIDIARerank\n",
        "\n",
        "NV_rerank = NVIDIARerank(model='nvidia/nv-rerankqa-mistral-4b-v3', top_n=10)"
      ],
      "metadata": {
        "id": "53t0R8DYpwf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Launching the LLM"
      ],
      "metadata": {
        "id": "jqq9gu-7bcfb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To complete our pipeline, let's connect our retrieval tools to the LLM that will give our chatbot life. Let's deploy an LLM NIM from the NVIDIA AI Foundation Models.\n",
        "\n",
        "To streamline this process, we can use **ChatNVIDIA** - a specialized module from LangChain that simplifies connecting to and interacting with NVIDIA's NIM APIs. **ChatNVIDIA** makes it easy to send queries, process responses, and manage inference settings without needing to handle raw API calls manually.\n",
        "\n",
        "Using the `available_models` function shows you all the available models you can use. Notice that there are models available from NVIDIA, Google, Meta, Microsoft, Mistral, and others! These are constantly being updated.\n"
      ],
      "metadata": {
        "id": "C4_a1KfZ2NCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "available_models = ChatNVIDIA.get_available_models()\n",
        "\n",
        "# Extracting and printing just the model names\n",
        "model_names = [model.id for model in available_models]\n",
        "print(\"\\n\".join(model_names))"
      ],
      "metadata": {
        "id": "3T3bO3nx0zu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For our purposes today, we're going to use Meta's `Llama 3.1-8b-instruct` model. This is a great, general purpose LLM that's particularly popular for simple chatbots. It's a small LLM at only 8 billion parameters. But don't let that fool you - it's very powerful!\n",
        "\n",
        "The model and its corresponding hyperparameters can be customized.\n",
        "\n",
        "* **Temperature** controls the randomness of the model's responses. A lower temperature makes the output more focused and deterministic, while a higher temperature makes it more creative and diverse.\n",
        "* **Top-p** (or nucleus sampling) determines the diversity of the generated text by considering only the smallest set of top predictions whose cumulative probability is at least p. It ensures that the model picks from a broader range of possible next words.\n",
        "* **Max tokens** limits the length of the output by specifying the maximum number of tokens (words or word pieces) the model can generate in a single response. Since we only get free credits for 1,000 tokens - best we keep this really small!"
      ],
      "metadata": {
        "id": "Z2i2s8qS5WQ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "foundation_model=\"meta/llama-3.1-8b-instruct\"\n",
        "llm = ChatNVIDIA(model=foundation_model, temperature=0.1, max_tokens=100, top_p=1.0)| StrOutputParser()"
      ],
      "metadata": {
        "id": "1vFBkPGa5Ulq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our LLM up and running - let's ask it something very basic to see if it's working..."
      ],
      "metadata": {
        "id": "Wq1TZFGV52CU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is the meaning of NVIDIA?\"\n",
        "\n",
        "answer = llm.invoke(question)\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "xCk1bHOjhW9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That was an easy one. Let's try asking it something from the knowledge base we created earlier with the NVIDIA Base Command Manager documentation."
      ],
      "metadata": {
        "id": "sNNZIdAmiSCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"When using qstat to monitor the status of a job, what does a E R job state mean?\"\n",
        "\n",
        "answer = llm.invoke(question)\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "8xHft6136AGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As expected, the foundational model won't be able to answer this question since it wasn't in it's original training dataset and it can't access external data. It's also worth noting that this was a purposefully tricky question which references data in both a table and formatted code listed below it.\n",
        "\n",
        "<img src=\"https://i.ibb.co/qMpSrmms/BCM-qstat.png\" width=\"100%\">\n",
        "\n",
        "To get all this to work, we need to connect our retrieval components — including the embedder, retriever, reranker, and LLM — into a structured workflow.\n",
        "\n",
        "This is where a **chain** comes in. In **LangChain**, a chain is a sequence of modular components that pass data through a defined pipeline. By chaining our retrieval steps together, we can finish the backend of our application."
      ],
      "metadata": {
        "id": "nfhzq9pGjqku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema import StrOutputParser\n",
        "from langchain.schema.runnable import Runnable, RunnablePassthrough, RunnableConfig\n",
        "from langchain_core.runnables import RunnableParallel\n",
        "\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "\n",
        "llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\")\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_kwargs={'k':100})\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"Answer solely based on the following context:\\n<Documents>\\n{context}\\n</Documents>\",\n",
        "        ),\n",
        "        (\"user\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "reranker = lambda input: NV_rerank.compress_documents(query=input['question'], documents=input['context'])\n",
        "\n",
        "chain = (\n",
        "    RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})\n",
        "    | {\"context\": reranker, \"question\": lambda input: input['question']}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "BQnoBI2b0iMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke(question)"
      ],
      "metadata": {
        "id": "gyzbCmkEDbbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now with the retrieval pipeline in place, we have our correct answer! Finally let's move to the last section where we can create a nice UI that allows us to easily interact with the RAGbot."
      ],
      "metadata": {
        "id": "f2HOQxlRGoN0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the Frontend"
      ],
      "metadata": {
        "id": "uFSuvtVoh0nF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's build a professional GUI for our RAGBot using **Chainlit**, a open-source framework that enables us to create interactive visual interfaces through Python code."
      ],
      "metadata": {
        "id": "Di5lihKti0yN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Theming\n",
        "We'll start by setting up a configuration file to customize our application's appearance and branding. This involves creating a public directory for assets and generating a `config.toml` file with both dark and light theme settings, along with implementing our logo and favicon."
      ],
      "metadata": {
        "id": "hk0JUXiAL4bI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Create theming for UI\n",
        "\n",
        "# Create the /public directory if it doesn't exist\n",
        "os.makedirs(\"public\", exist_ok=True)\n",
        "\n",
        "# Create config.toml if it doesn't exist\n",
        "if not os.path.exists(\"config.toml\"):\n",
        "    with open(\"config.toml\", \"w\") as f:\n",
        "      f.write(\"\"\"\\n\n",
        "# ==============================================================================\n",
        "######################### CHAINLIT CONFIGURATION FILE ##########################\n",
        "# ==============================================================================\n",
        "\n",
        "### GENERAL SETTINGS ===========================================================\n",
        "### Alter general settings of the app ==========================================\n",
        "### ============================================================================\n",
        "\n",
        "[UI]\n",
        "# App Name\n",
        "name = \"NVIDIA RAGBot\"\n",
        "\n",
        "# CSS File\n",
        "# > Specify a CSS file that can be used to customize the user interface. The CSS\n",
        "#   file can be served from the public directory or via an external link.\n",
        "custom_css = \"/public/stylesheet.css\"\n",
        "\n",
        "### DEFAULT THEME COLOURS ======================================================\n",
        "### Change your app's default colour palette ===================================\n",
        "# > Background colour: Change the colour of the app’s background.\n",
        "# > Paper colour: Alters the colour of the ‘paper’ elements within\n",
        "#   the app, such as the navbar, widgets, etc.\n",
        "# > Primary colour: Encompasses three shades - main, dark, and light. These\n",
        "#   colours are primarily used for interactive interface elements.\n",
        "### ============================================================================\n",
        "\n",
        "[UI.theme]\n",
        "default = \"dark\"\n",
        "layout = \"wide\"\n",
        "font_family = \"Inter, sans-serif\"\n",
        "\n",
        "# Modify Light Theme\n",
        "[UI.theme.light]\n",
        "background = \"#FAFAFA\"\n",
        "paper = \"#EBEEEF\"\n",
        "\n",
        "[UI.theme.light.primary]\n",
        "main = \"#76B900\"\n",
        "dark = \"#76B900\"\n",
        "light = \"#333333\"\n",
        "\n",
        "# Modify Dark Theme\n",
        "[UI.theme.dark]\n",
        "#background = \"#FAFAFA\"\n",
        "#paper = \"#FFFFFF\"\n",
        "\n",
        "[UI.theme.dark.primary]\n",
        "main = \"#76B900\"\n",
        "dark = \"#1A1A1A\"\n",
        "light = \"#333333\"\n",
        "\n",
        "\n",
        "\"\"\")\n",
        "    print(\"config.toml created successfully!\")\n",
        "\n",
        "# Download the dark and light mode logos\n",
        "!wget -O \"public/logo_dark.png\" -nc https://i.ibb.co/SPWqM8V/nvidia-logo-horizontal-white.png\n",
        "!wget -O \"public/logo_light.png\" -nc https://i.ibb.co/3dzk4pH/nvidia-logo-horizontal-colour.png\n",
        "\n",
        "# Download the favicon\n",
        "!wget -O \"public/favicon.ico\" -nc https://nvidianews.nvidia.com/media/sites/219/images/favicon.ico\n",
        "\n",
        "print(\"Logos and favicon downloaded successfully!\")"
      ],
      "metadata": {
        "id": "bsuhECIsjONR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chainlit\n",
        "Now let's get the code to create a Chainlit-powered frontend connected to the backend that we've already put together. As much of this code is boilerplate and outside of the scope of this tutorial, we're going to download the full file to our system. But if you'd like to see the code itself, simply click on the `app.py` file in the directory after running this cell!"
      ],
      "metadata": {
        "id": "DhOGcOLELqJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download frontend code from GitLab and save as app.py\n",
        "!wget -nc -O app.py https://gitlab.com/adaveinthelife/external_blueprints/-/raw/main/app_files/Enterprise_RAG_BP_frontend_EXT.txt"
      ],
      "metadata": {
        "id": "RObTiSIMoHTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tunneling and Deploying the Interface\n",
        "\n",
        "Finally the last step where we bring the frontend and backend together to make a full Enterprise RAG application. To deploy our application, we'll run Chainlit but since we don't have it hosted on a webserver, we'll make it accessible from our notebook through a temporary public URL. After running this cell, you'll receive a unique URL that we can use to easily interact and demonstrate our application.\n",
        "\n",
        "Remember that this application will stay active until the cell is stopped or the notebook is closed.\n",
        "\n",
        "This final step combines our frontend and backend into a complete Enterprise RAG system. Since we're working in a notebook environment, we'll create a temporary public URL using an open-source tool named `Tunnelmole` to make our application accessible. After running the cell below, you'll receive a unique link where you can interact with and demonstrate your application.\n",
        "\n",
        "**Remember** - the system will remain active until you stop the cell or close the notebook."
      ],
      "metadata": {
        "id": "YQBrZZpZNUgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import time\n",
        "import re\n",
        "\n",
        "print(\"🟢 Initializing system...\")\n",
        "\n",
        "# Start Chainlit in the background\n",
        "chainlit_process = subprocess.Popen(\n",
        "    [\"chainlit\", \"run\", \"app.py\", \"--headless\", \"--port\", \"8000\"],\n",
        "    stdout=subprocess.DEVNULL,\n",
        "    stderr=subprocess.DEVNULL\n",
        ")\n",
        "\n",
        "# Wait for Chainlit to initialize\n",
        "time.sleep(10)\n",
        "\n",
        "def start_tunnelmole():\n",
        "    \"\"\"Starts Tunnelmole and extracts only the public HTTPS URL.\"\"\"\n",
        "    tunnel_process = subprocess.Popen(\n",
        "        [\"tunnelmole\", \"8000\"],\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        text=True\n",
        "    )\n",
        "\n",
        "    public_url = None\n",
        "    for line in tunnel_process.stdout:\n",
        "        match = re.search(r\"(https://[\\w-]+\\.tunnelmole\\.net)\", line)\n",
        "        if match:\n",
        "            public_url = match.group(1)\n",
        "            break  # Stop once URL is found\n",
        "\n",
        "    return public_url\n",
        "\n",
        "# Start Tunnelmole\n",
        "public_url = start_tunnelmole()\n",
        "\n",
        "if public_url:\n",
        "    print(f\"\\n🚀 Access your application here: {public_url}\\n\")\n",
        "    print(\"\\n🔹 This session will remain active until you manually stop it.\")\n",
        "    print(\"🔹 To stop the session, either:\")\n",
        "    print(\"   - Press the 'Stop' button in the top left corner of the cell.\")\n",
        "    print(\"   - Close the notebook.\")\n",
        "else:\n",
        "    print(\"\\n❌ Failed to start Tunnelmole.\\n\")\n",
        "\n",
        "\n",
        "# Keep the session alive\n",
        "while True:\n",
        "    time.sleep(60)"
      ],
      "metadata": {
        "id": "tt1qErbPofWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What's Next? ,🚀\n",
        "Thank you for taking the time to complete this course and workbook! We hope it has been informative and given you practical experience in building advanced AI solutions, especially in applying Retrieval-Augmented Generation (RAG) techniques to enhance chatbot capabilities. If you're interested in furthering your knowledge of NVIDIA AI technologies, we encourage you to explore additional resources:\n",
        "\n",
        "*   [Explore NVIDIA's Foundational Models page](https://build.nvidia.com/explore/discover) to explore a growing library of cutting-edge AI models running in NVIDIA NIMs. You can demo each model from your browser allowing you to experiment with their capabilities and easily integrate them into your projects.\n",
        "*   [Visit the NVIDIA Developer Blog](https://developer.nvidia.com/blog) for the latest tutorials and articles on AI, data science, and GPU-accelerated computing.\n",
        "*   [Check out the NVIDIA AI Deep Learning Institute](https://www.nvidia.com/en-us/training/) to dive deeper into topics like deep learning, model optimization, and more.\n",
        "*   [Join the NVIDIA Developer Program](https://developer.nvidia.com/) to access exclusive tools, software development kits (SDKs), and forums where you can connect with experts in the AI field.\n",
        "\n",
        "By staying connected with the NVIDIA ecosystem, you'll be able to keep up with cutting-edge advancements and continue honing your skills in AI.\n",
        "\n",
        "Once again, congratulations on your achievement, and thank you for being part of this exciting journey into the world of Generative AI!"
      ],
      "metadata": {
        "id": "7Wuv-Te6QTMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''                   ======================================\n",
        "                      ======================================\n",
        "                      ======================================\n",
        "               ========        =============================\n",
        "           ==================      =========================\n",
        "        =======       ===========      =====================\n",
        "     =======      +====   ==========     ===================\n",
        "  =======+     ========        =======     =================\n",
        "========    ======+   ====       =======     ===============\n",
        "=======   ======      ======    =======     ================\n",
        " =======   =====+     ======= ========    ==================\n",
        "  =======   ======    ==============    ====================\n",
        "   +======   +=====   ===========+    =========     ========\n",
        "     ======    +==============     =========           =====\n",
        "      =======     =====        ==========           ========\n",
        "        +=======      ===============+          +===========\n",
        "           =========  ===========          =================\n",
        "              =========              =======================\n",
        "                   +===     ================================\n",
        "                      ======================================\n",
        "                      ======================================\n",
        "'''\n",
        "print(\"Until next time!\")"
      ],
      "metadata": {
        "id": "KeZ_N8DucN_U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}